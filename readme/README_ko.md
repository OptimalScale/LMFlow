<p align="center" width="100%">
<img src="../assets/logo.png" alt="LMFlow" style="width: 100%; min-width: 300px; display: block; margin: auto; background-color: transparent;">
</p>

# LMFlow

<h4 align="center">
    <p>
        <a href="https://github.com/OptimalScale/LMFlow/blob/main/README.md">English</a> |
        <a href="https://github.com/OptimalScale/LMFlow/blob/main/readme/README_zh-hans.md">ç®€ä½“ä¸­æ–‡</a> |
        <a href="https://github.com/OptimalScale/LMFlow/blob/main/readme/README_es.md">EspaÃ±ol</a> |
        <a href="https://github.com/OptimalScale/LMFlow/blob/main/readme/README_jp.md">æ—¥æœ¬èª</a> |
        <b>í•œêµ­ì–´</b> |
        <a href="https://github.com/OptimalScale/LMFlow/blob/main/readme/README_hindi.md">à¤¹à¤¿à¤‚à¤¦à¥€</a>
    <p>
</h4>

> [!NOTE]
> The Korean README file was translated by LLM for reference only. Korean speakers are welcome to submit a PR to polish the document!  

> [!NOTE]  
> í•œêµ­ì–´ README íŒŒì¼ì€ ì°¸ê³ ìš©ìœ¼ë¡œ LLMì— ì˜í•´ ë²ˆì—­ë˜ì—ˆìŠµë‹ˆë‹¤. í•œêµ­ì–´ ì‚¬ìš©ìë“¤ì€ ë¬¸ì„œë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´ PRì„ ì œì¶œí•  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤!  

[![Website](https://img.shields.io/badge/Website-Demo-20B2AA.svg)](https://lmflow.com)
[![Code License](https://img.shields.io/badge/Code%20License-Apache_2.0-green.svg)](https://github.com/OptimalScale/LMFlow/blob/main/LICENSE)
[![Python 3.9+](https://img.shields.io/badge/Python-3.9+-blue.svg)](https://www.python.org/downloads/release/python-390/)
[![Doc](https://img.shields.io/badge/Website-Doc-ff69b4.svg)](https://optimalscale.github.io/LMFlow/)
[![Embark](https://img.shields.io/badge/Discord-LMFlow-%237289da.svg?logo=discord)](https://discord.gg/u9VJNpzhvA)
[![slack badge](https://img.shields.io/badge/Slack-Join-blueviolet?logo=slack&amp)](https://join.slack.com/t/lmflow/shared_invite/zt-1wju9nicy-woXbNtS~5MavHSAtiMxmxQ)
[![WeChat badge](https://img.shields.io/badge/WeChat-Join-brightgreen?logo=wechat&amp)](https://ibb.co/ZhM4hhn)

ë‹¤ìŒì€ ì‚¬ìš©ì ì¹œí™”ì ì´ê³  ë¹ ë¥´ë©° ì‹ ë¢°í•  ìˆ˜ ìˆìœ¼ë©° ì»¤ë®¤ë‹ˆí‹° ì „ì²´ì— ì•¡ì„¸ìŠ¤í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ëœ ëŒ€ê·œëª¨ ê¸°ê³„ í•™ìŠµ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ë° ìœ ìš©í•œ í™•ì¥ ê°€ëŠ¥í•˜ê³  í¸ë¦¬í•˜ë©° íš¨ìœ¨ì ì¸ ë„êµ¬ ìƒìì…ë‹ˆë‹¤.

<p align="center" width="100%">
<img src="../assets/features.png" alt="LMFlow-features" style="width: 100%; min-width: 300px; display: block; margin: auto;">
</p>


## ìµœì‹  ë‰´ìŠ¤
* [2024-03-27] :rocket: [LISA](https://arxiv.org/abs/2403.17919)ë¥¼ ì§€ì›í•©ë‹ˆë‹¤. ë©”ëª¨ë¦¬ë¥¼ ë¹„ìš°ì§€ ì•Šê³ ë„ 24G ë©”ëª¨ë¦¬ì—ì„œ 7B í›ˆë ¨ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤! :rocket:
* [2023-09-11] [ì¶”ë¡ ì  ë””ì½”ë”© (speculative decoding)](https://arxiv.org/abs/2211.17192)ì„ ì§€ì›í•©ë‹ˆë‹¤. ì‚¬ìš©ë²• ë° ê°€ì†í™” ì„¸ë¶€ ì •ë³´ëŠ” [speculative_decoding](https://github.com/OptimalScale/LMFlow/blob/main/scripts/speculative_decoding/README.md) ë¥¼ í™•ì¸í•˜ì„¸ìš”.
* [2023-08-14] LLaMA ëª¨ë¸ì— ëŒ€í•œ ìœ„ì¹˜ ë³´ê°„(ì„ í˜• ë° NTK ìŠ¤ì¼€ì¼ë§)ì„ ì‚¬ìš©í•˜ì—¬ ê¸´ ë¬¸ë§¥ ì¶”ë¡ ì„ ì§€ì›í•©ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [Postion Interpolation](https://github.com/OptimalScale/LMFlow/blob/main/readme/Position_Interpolation.md) ë¥¼ í™•ì¸í•˜ì„¸ìš”.
* [2023-08-07] [Flash Attention-2](https://crfm.stanford.edu/2023/07/17/flash2.html)ë¥¼ ì§€ì›í•©ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [Flash Attention](https://github.com/OptimalScale/LMFlow/blob/main/readme/flash_attn2.md) ë¥¼ í™•ì¸í•˜ì„¸ìš”.
* [2023-08-02] [Llama2](https://ai.meta.com/llama/), [ChatGLM2](https://huggingface.co/THUDM/chatglm2-6b) ë° [Baichuan](https://huggingface.co/baichuan-inc/Baichuan-7B) ëª¨ë¸ì„ ì§€ì›í•©ë‹ˆë‹¤.


## ë¹ ë¥¸ ì‹œì‘
### ì„¤ì¹˜
ì €í¬ì˜ RepoëŠ” ì´ë¯¸ ë¦¬ëˆ…ìŠ¤ (ìš°ë¶„íˆ¬ 20.04)ì—ì„œ ì™„ì „í•œ í…ŒìŠ¤íŠ¸ê°€ ì´ë£¨ì–´ì¡ŒìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ìš´ì˜ ì²´ì œ í”Œë«í¼ (ë§¥OS, ìœˆë„ìš°)ì€ ì•„ì§ ì™„ì „íˆ í…ŒìŠ¤íŠ¸ë˜ì§€ ì•Šì•˜ìœ¼ë¯€ë¡œ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë¨¼ì € ë¦¬ëˆ…ìŠ¤/ìœˆë„ìš° WSLì—ì„œ ì‚¬ìš©í•´ë³´ê±°ë‚˜ Google Colabì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.
CUDA 10.3-11.7ì— ëŒ€í•´ì„œëŠ” `v0.0.5` ë° ê·¸ ì´ì „ ë²„ì „ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. 11.7ë³´ë‹¤ í° CUDAì˜ ê²½ìš°, ë” ë‚˜ì€ ê²½í—˜ì„ ìœ„í•´ ìš°ë¦¬ì˜ stable ë¸Œëœì¹˜ì¸ `>= v0.0.6` ì„ ì‚¬ìš©í•˜ì‹­ì‹œì˜¤.
```bash
git clone https://github.com/OptimalScale/LMFlow.git
cd LMFlow
conda create -n lmflow python=3.9 -y
conda activate lmflow
conda install mpi4py
bash install.sh
```

### ë°ì´í„°ì…‹ ì¤€ë¹„
ì €í¬ì˜ [ê³µì‹ ë¬¸ì„œ(ì˜ë¬¸)](https://optimalscale.github.io/LMFlow/examples/DATASETS.html) ë¥¼ ì°¸ê³ í•´ ì£¼ì„¸ìš”. ê³µì‹ ë¬¸ì„œëŠ” í˜„ì¬ ë²ˆì—­ ì¤‘ì´ë©°, ì¡°ê¸ˆë§Œ ê¸°ë‹¤ë ¤ ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤.

### íŒŒì¸ íŠœë‹ (ì „ì²´ ë§¤ê°œë³€ìˆ˜)
> [!IMPORTANT]
> ìµœê·¼ì— ë°ì´í„° ì €ì¥ ì„œë²„ì— ì¼ë¶€ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œí•  ë•Œ, ìµœì‹  ìŠ¤í¬ë¦½íŠ¸ì¸ ë©”ì¸ ë¸Œëœì¹˜ì˜ [`download.sh`](https://github.com/OptimalScale/LMFlow/blob/main/data/download.sh) ë¥¼ ì‚¬ìš©í•´ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤. ë¶ˆí¸ì„ ë¼ì³ë“œë ¤ ì£„ì†¡í•©ë‹ˆë‹¤.

ì „ì²´ ë§¤ê°œë³€ìˆ˜ íŒŒì¸ íŠœë‹ì€ ëª¨ë¸ì˜ ëª¨ë“  ë§¤ê°œë³€ìˆ˜ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤. GPT-2ì˜ ì „ì²´ ë§¤ê°œë³€ìˆ˜ íŒŒì¸ íŠœë‹ì˜ ì˜ˆì‹œëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤:
```sh
cd data && ./download.sh alpaca && cd -

./scripts/run_finetune.sh \
  --model_name_or_path gpt2 \
  --dataset_path data/alpaca/train \
  --output_model_path output_models/finetuned_gpt2
```

### Online Service
> LMflowì˜ [ì›¹ ì„œë¹„ìŠ¤ë¥¼](https://lmflow.com/) ë°©ë¬¸í•´ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤. LMflowì˜ ì›¹ì‚¬ì´íŠ¸ì— LLaMA-7B-tunedì™€ LLaMA-33B-tunedë¥¼ ë¯¸ë¦¬ ë°°í¬í•´ ë†“ì•˜ìŠµë‹ˆë‹¤.  ì›¹ì‚¬ì´íŠ¸ íŠ¸ë˜í”½ì´ ë§ì„ ê²½ìš°, ì›¹ì‚¬ì´íŠ¸ê°€ ì ì ˆí•˜ê²Œ ì‘ë‹µí•˜ì§€ ì•Šì„ ìˆ˜ ìˆì§€ë§Œ, ì›¹ ì„œë¹„ìŠ¤ì˜ `Local Deploy`ë¥¼ ì°¸ì¡°í•˜ì—¬ ì§ì ‘ ë°°í¬í•´ë³´ì‹¤ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.

### Colab chatbot(shell)
<p align="center" width="100%">
<img src="../assets/colab-shell-chatbot-demo.png">
</p>

LMflowëŠ” êµ¬ê¸€ ì½”ë©ì˜ T4/P100/V100 GPUë¥¼ ì´ìš©í•œ ê°„ë‹¨í•œ ì‰˜ ì±—ë´‡ ë°ëª¨ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ë°ëª¨ë¡œ ì œê³µë˜ëŠ” `gpt-neo-2.7b` ëª¨ë¸ì€ ì˜ì–´ë¡œë§Œ ì‚¬ìš©í•˜ì‹¤ ìˆ˜ ìˆê³ , ë‹¤ë¥¸ LLM ëª¨ë¸ì— ë¹„í•´ ì„±ëŠ¥ì´ ë›°ì–´ë‚˜ì§€ ì•Šì€ ë°ëª¨ìš© ëª¨ë¸ì„ìœ¼ë¡œ ì°¸ê³ ë§Œ í•´ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤. ìœ ì €ëŠ” LMFlowì„ í†µí•´ ìì‹ ì˜ ë°ì´í„°ì…‹ì— ëª¨ë¸ì„ íŒŒì¸íŠœë‹í•˜ê³  ë” ë‚˜ì€ ì„±ëŠ¥ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ, ğŸ¤—[huggingface](https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads)ì—ì„œ ì œê³µí•˜ëŠ” ë‹¤ë¥¸ decoder-only ëª¨ë¸ë“¤ ë˜í•œ ë‹¤ìŒê³¼ ê°™ì´ íŒŒì¸íŠœë‹ í•´ë³´ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```sh
./scripts/run_chatbot.sh {another-model-name}
```




### Colab chatbot(web)
LMflowëŠ” êµ¬ê¸€ ì½”ë©ì˜ T4/P100/V100 GPUë¥¼ ì´ìš©í•œ ê°„ë‹¨í•œ ì›¹ ë°ëª¨ ì±—ë´‡ì„ ì œê³µí•©ë‹ˆë‹¤. ë°ëª¨ë¡œ ì œê³µë˜ëŠ” `gpt-neo-2.7b` ëª¨ë¸ì€ ì˜ì–´ë¡œë§Œ ì‚¬ìš©í•˜ì‹¤ ìˆ˜ ìˆê³ , ë‹¤ë¥¸ LLM ëª¨ë¸ì— ë¹„í•´ ì„±ëŠ¥ì´ ë›°ì–´ë‚˜ì§€ ì•Šì€ ë°ëª¨ìš© ëª¨ë¸ì„ìœ¼ë¡œ ì°¸ê³ ë§Œ í•´ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤.

### Local Deploy
ì¶©ë¶„í•œ ë¡œì»¬ ë¦¬ì†ŒìŠ¤ê°€ ìˆê³ , ëª¨ë¸ì„ ë¡œì»¬ì—ì„œ ë°°í¬í•˜ê³  ì‹¶ì–´í•˜ëŠ” ìœ ì €ë¥¼ ìœ„í•´, LMflowëŠ” ë°±ì—”ë“œ(ë‹¤ë¥¸ í”„ë¡ íŠ¸ì—”ë“œì— ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•˜ê¸° ìœ„í•´)ì™€ interactive ì›¹ í”„ë¡ íŠ¸ì—”ë“œ(ì§ì ‘ ëŒ€í™”í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ”)ì˜ launchë¥¼ ìœ„í•œ í”Œë¼ìŠ¤í¬ ì„œë²„ë¥¼ ì‰½ê²Œ ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì„ ì œê³µí•©ë‹ˆë‹¤. 
```sh
cd ./service
python app.py
```

## Medical Performance
|                |  PubMedQA (ID) | MedQA-USMLE (OOD) | MedMCQA (ID) |  Average |
|:---------:|:--------:|:-----------:|:-------:|:----:|
| Human (pass)   |  60.0   |     50.0    |         |      |
| Human (expert) |    78.0   |     87.0    |  90.0   | 85.0 |
|   |      |              |    |  |
|  InstructGPT 175B   |   73.2   |     46.0    |  44.0   | 54.4 |
|    ChatGPT |    63.9   |     **57.0**    |  44.7   | 55.2 |
|      LLaMA 7B   |    5.2   |     27.1    |  24.3   | 18.9 |
|      LLaMA 33B |    1.8   |     43.4    |  30.3   | 25.2 |
|   |      |             |            |    |  |
|   Task-tuned LLaMA 7B (Full) |   **75.1**   |     44.5    |  49.9   | 56.5 |
| Task-tuned LLaMA 33B (LoRA) |  74.0  |  51.3   | **50.2**|**58.5**|


LLaMA 33B (LoRA)ì˜ ì„±ëŠ¥ì€ ë‹¨ì¼ 8 \* A100 ì„œë²„ë¡œ PubMedQAì™€ MedMCQAì„ ì‚¬ìš©í•˜ì—¬, 16ì‹œê°„ë™ì•ˆ íŒŒì¸íŠœë‹í•œ ê²°ê³¼ì…ë‹ˆë‹¤. Instruction tuning ë“±ì„ í¬í•¨í•´ ë” ë§ì€ ëª¨ë¸ ì„±ëŠ¥ì— ëŒ€í•´ì„œ ì•Œê³  ì‹¶ìœ¼ì‹œë©´, ë‹¤ìŒ [ë¬¸ì„œ](https://optimalscale.github.io/LMFlow/)ë¥¼ ì°¸ì¡°í•´ì£¼ì„¸ìš”.

## Model Zoo
LMflowëŠ” í•™ìŠµëœ ì²´í¬í¬ì¸íŠ¸ë“¤ì„ í†µí•´ ì¶”ê°€ í•™ìŠµ ë° ì¶”ë¡ í•˜ì‹¤ ìˆ˜ ìˆë„ë¡ ëª¨ë“  ëª¨ë¸ì„ ì˜¤í”ˆì†ŒìŠ¤ë¡œ ì œê³µí•©ë‹ˆë‹¤.

| Instruct-tuned Models   |  Status | Base Model | Download |
|----------|:-------------:|----------|:-------------:|
| LLaMA-7B-tuned | ![completed](https://geps.dev/progress/100) | LLaMA-7B | [Google Drive](https://drive.google.com/file/d/1x5JLae3akVkfFeDhSe3TEyUbPn_GNFyb/view?usp=share_link) |
| LLaMA-13B-tuned | ![completed](https://geps.dev/progress/100) | LLaMA-13B |  [Google Drive](https://drive.google.com/file/d/1m_rpe6rNpN59kWvjJ3GfKeEmS-68TRYr/view?usp=share_link) |
| LLaMA-33B-tuned | ![completed](https://geps.dev/progress/100) |LLaMA-33B |  [Google Drive](https://drive.google.com/file/d/1IqgqLHwNkWQ7BffheZnqD6a-8Zul1bk6/view?usp=share_link) |
| LLaMA-65B-tuned | ![training](https://geps.dev/progress/65) | LLaMA-65B | Google Drive |
| LLaMA7B-medical | ![completed](https://geps.dev/progress/100) | LLaMA-7B | [Google Drive](https://drive.google.com/file/d/1Z44tsrRvfDFvucbNGFjHC_vbPcBvg3x-/view?usp=share_link) |
| LLaMA13B-medical | ![completed](https://geps.dev/progress/100) | LLaMA-13B |  [Google Drive](https://drive.google.com/file/d/1uoTAXTMyYQkP6N4ummx7tj-c4v1p91ap/view?usp=share_link) |
| LLaMA33B-medical | ![completed](https://geps.dev/progress/100) |LLaMA-33B |  [Google Drive](https://drive.google.com/file/d/14N9o_1pwHmVuSikQ3orMVzZDrLYJC0iM/view?usp=share_link) |
| LLaMA65B-medical | ![training](https://geps.dev/progress/90) | LLaMA-65B | Google Drive |


## Supported Pipelines

| Pipelines   |   Status |
|----------|:-------------:|
| Task Tuning |  :white_check_mark: Supported |
| Instruction Tuning |  :white_check_mark: Supported |
| Parameter-Efficient Tuning |  :white_check_mark: Supported |
| Large Model Inference |  :white_check_mark: Supported |
| Alignment Tuning |  :wrench: Developing |



## Supported Models

ğŸ¤— huggingfaceì˜ ëª¨ë“  [ë””ì½”ë” ëª¨ë¸](https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads)ì— LMflowë¥¼ ì›í™œí•˜ê²Œ ì ìš©í•´ë³´ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. LLaMA, GPT2, GPT-Neo, Galactica ë“±ì€ ì™„ë²½í•˜ê²Œ í…ŒìŠ¤íŠ¸ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì¶”í›„ LMflowëŠ” ì¸ì½”ë” ëª¨ë¸ë„ ì§€ì›í•  ì˜ˆì •ì…ë‹ˆë‹¤.


## 1.Setup

ì†Œí”„íŠ¸ì›¨ì–´ íŒ¨í‚¤ì§€ëŠ” Linux ìš´ì˜ ì²´ì œ(Ubuntu 20.04)ì—ì„œ ì™„ë²½í•˜ê²Œ í…ŒìŠ¤íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ìš´ì˜ ì²´ì œ í”Œë«í¼(MacOS, Windows)ì€ í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰í•˜ê³  ìˆìœ¼ë¯€ë¡œ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Linux ì‹œìŠ¤í…œì—ì„œ ì‚¬ìš©í•˜ì‹œëŠ” ê²ƒì„ ì¶”ì²œë“œë¦¬ê³ , Google Colabì„ í†µí•´ í…ŒìŠ¤íŠ¸í•´ë³´ì‹œê¸° ë°”ëë‹ˆë‹¤.

```bash
git clone https://github.com/OptimalScale/LMFlow.git
cd LMFlow
conda create -n lmflow python=3.9 -y
conda activate lmflow
conda install mpi4py
pip install -e .
```

## 2.Prepare Dataset
ë‹¤ìŒì„ ì‹¤í–‰í•˜ë©´ ì˜ˆì œ í•™ìŠµ ë°ì´í„°ì…‹ê³¼ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì„ ì‰½ê²Œ ë‹¤ìš´ë¡œë“œí•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
```bash
cd data
bash download.sh all
cd -
```

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ê°„ë‹¨íˆ ë³€í™˜í•˜ë©´ ìì‹ ì˜ ë°ì´í„°ì…‹ì„ ëª¨ë¸í•™ìŠµì— ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
```json
{
  "type": "text2text",
  "instances": [
    {
      "input": "Question: The Transformer architecture [START_REF]",
      "output": "N/A"
    },
    ...
  ]
}
```
```json
{
  "type": "text_only",
  "instances": [
    {
      "text": "Defintion: In this task, we ask you to write an answer to a question that involves events that may be stationary (not changing over time) or transient (changing over time). For example, the sentence \"he was born in the U.S.\" contains a stationary event since it will last forever; however, \"he is hungry\" contains a transient event since it will remain true for a short period of time. Note that a lot of the questions could have more than one correct answer. We only need a single most-likely answer. Please try to keep your \"answer\" as simple as possible. Concise and simple \"answer\" is preferred over those complex and verbose ones. \n Input: Question: Sentence: It's hail crackled across the comm, and Tara spun to retake her seat at the helm. \nQuestion: Will the hail storm ever end? \n Output: NA \n\n"
    },
    ...
  ]
}
```
## 3. Run Scripts
### 3.1 Run íŒŒì¸íŠœë‹

`scripts/run_finetune.sh` ë¥¼ ì‹¤í–‰í•˜ì—¬ GPT-2 ë² ì´ìŠ¤ ëª¨ë¸ì„ íŒŒì¸íŠœë‹í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
```sh
./scripts/run_finetune.sh
```

deepspeedì— argumentsë¥¼ ì¶”ê°€ë¡œ ì…ë ¥í•˜ì‹œê³ ì í•  ê²½ìš°,
```sh
./scripts/run_finetune.sh "--num_gpus=8 --master_port 10001"
```

LoRA íŒŒì¸íŠœë‹ì„ í™œì„±í•˜ì‹œê³ ì í•  ê²½ìš°,
```sh
./scripts/run_finetune_with_lora.sh
```



ìì„¸í•œ ì„¤ì •ì€ ì´ ìŠ¤í¬ë¦½íŠ¸ë“¤ì„ ì§ì ‘ ìˆ˜ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ìŠ¤í¬ë¦½íŠ¸ë“¤ì€ ì‹¤ì œë¡œëŠ” íŒŒì´ì¬ ìŠ¤í¬ë¦½íŠ¸ `examples/finetune.py`ë¥¼ í˜¸ì¶œí•˜ë©°, ë‹¤ìŒê³¼ ê°™ì´ ì‚¬ìš©í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```sh
deepspeed ${deepspeed_args} \
  examples/finetune.py \
    --deepspeed configs/ds_config_zero3.json \
    --bf16 \
    --run_name finetune_with_lora \
    --model_name_or_path facebook/galactica-1.3b \
    --num_train_epochs 0.01 \
    --learning_rate 2e-5 \
    --dataset_path ${dataset_path} \
    --per_device_train_batch_size 1 \
    --per_device_eval_batch_size 1 \
    --validation_split_percentage 0 \
    --logging_steps 20 \
    --block_size 512 \
    --do_train \
    --output_dir output_models/finetune \
    --overwrite_output_dir \
    --ddp_timeout 72000 \
    --save_steps 5000 \
    --dataloader_num_workers 1
```
ì—¬ê¸°ì„œëŠ” `--num_train_epochs`ì˜ epoch ìˆ˜ë¥¼ `0.01` ë¡œ ì„¤ì •í•˜ì—¬ íŒŒì¸íŠœë‹ í”„ë¡œì„¸ìŠ¤ë¥¼ ë¹ ë¥´ê²Œ ì™„ë£Œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë” ë‚˜ì€ ì„±ëŠ¥ì˜ ëª¨ë¸ì„ ì–»ê³  ì‹¶ë‹¤ë©´ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•˜ì‹œë©´ ë©ë‹ˆë‹¤. 


ëª¨ë“  ê°€ëŠ¥í•œ íŒŒì¸íŠœë‹ argumentsë¥¼ í™•ì¸í•˜ì‹œë ¤ë©´,
```python
python examples/finetune.py -h
```

ì°¸ê³ ë¡œ í›ˆë ¨ ë°ì´í„° ì„¸íŠ¸ê°€ ì‘ì€ ê²½ìš° ``block_size`` ì˜ ê°’ì„ ë‚®ì¶°ì•¼ë§Œ í•©ë‹ˆë‹¤. ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ Epoch Iterationì—ì„œ ìƒ˜í”Œì„ ì‚¬ìš©í•  ìˆ˜ ì—†ê²Œë©ë‹ˆë‹¤.

íŒŒì¸íŠœë‹ ëœ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ëŠ” ìœ„ì˜ ì˜ˆì‹œì—ì„œ `--output_dir`ë¡œ ì§€ì •ëœ ì¸ìì— ì €ì¥ë©ë‹ˆë‹¤. 
ì´ ê²½ìš°ì—ëŠ”`output_models/finetune` ì…ë‹ˆë‹¤.

### 3.2 Run Evaluation

ê¸°ì¡´ huggingface ëª¨ë¸ë¡œ ì§ì ‘ Evaluationì„ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì˜ˆë¥¼ ë“¤ì–´ GPT2 largeë¥¼ ì‹¤í–‰í•˜ë ¤ë©´, ë‹¤ìŒì„ ì‰˜ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‚¬ìš©í•˜ì‹œê±°ë‚˜
```sh
./scripts/run_evaluation.sh
```
ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ í†µí•´ íŒŒì´ì¬ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ì‹­ì‹œì˜¤.
```python
CUDA_VISIBLE_DEVICES=0 \
    deepspeed examples/evaluate.py \
    --answer_type medmcqa \
    --model_name_or_path gpt2-large \
    --dataset_path data/MedQA-USMLE/validation \
    --deepspeed examples/ds_config.json
```
íŒŒì¸íŠœë‹ ëœ ëª¨ë¸ì„ ë¡œë“œí•˜ë ¤ë©´ ì €ì¥ëœ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ ê²½ë¡œë¥¼ `--model_name_or_path`ë¥¼ ì‚¬ìš©í•´ ì§€ì •í•˜ì‹­ì‹œì˜¤.

LoRA íŒŒì¸íŠœë‹ ëœ ëª¨ë¸ì˜ ê²½ìš° ë‹¤ìŒì„ ì°¸ì¡°í•˜ì‹­ì‹œì˜¤.
```sh
./scripts/run_evaluation_with_lora.sh
```

ì´ëŸ¬í•œ ìŠ¤í¬ë¦½íŠ¸ëŠ” ì €í¬ì˜ APIë¥¼ ê¸°ë°˜ìœ¼ë¡œ êµ¬ì¶•ëœ ì˜ˆì œ `examples/*.py` ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤. 
ë” ë§ì€ API ê´€ë ¨ ì˜ˆì œëŠ” unittestì˜ `tests` ë©”ì†Œë“œë¥¼ ì°¸ì¡°í•˜ì‹­ì‹œì˜¤.

## 4. Additional Notes
### 4.1 LLaMA Checkpoint

1. ë¨¼ì € [facebookresearch/llama](https://github.com/facebookresearch/llama)ì—ì„œ LLaMA ëª¨ë¸ì— ëŒ€í•œ ì•¡ì„¸ìŠ¤ ê¶Œí•œì„ ì–»ì–´ì•¼í•©ë‹ˆë‹¤. ê³µì‹ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  ê²½ë¡œë¥¼ `${llama-path}`ì— ì €ì¥í•˜ì‹­ì‹œì˜¤.

2. ì•„ë˜ì˜ ì»¤ë§¨ë“œë¥¼ ì‹¤í–‰í•˜ì—¬ ê³µì‹ ì²´í¬í¬ì¸íŠ¸ `${llama-path}`ë¥¼ HuggingFaceê°€ ì§€ì›í•˜ëŠ” ì²´í¬í¬ì¸íŠ¸ `${llama-hf-path}`ë¡œ ë³€í™˜í•˜ì‹­ì‹œì˜¤.

    `python ./scripts/convert_llama_weights_to_hf.py --input_dir ${llama-path} --model_size 7B --output_dir ${llama-hf-path}/llama-7b-hf`

3. ê·¸ ë‹¤ìŒì— `${llama-hf-path}/llama-7b-hf`ë¡œ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œë¥¼ ì„¤ì •í•˜ì‹œë©´ ëì…ë‹ˆë‹¤.

4. (ì„ íƒ ì‚¬í•­) ì˜¤ë¦¬ì§€ë„ llama-7b-hf Pre-trained ëª¨ë¸ ì „ë¶€ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ì‹œê³ ì í•  ê²½ìš° ë‹¤ìŒì„ ì‹¤í–‰í•˜ì„¸ìš”.
```sh
cd output_models && ./download.sh all && cd -
```
`./scripts/run_evaluation_with_lora.sh`ì™€ ìœ ì‚¬í•œ ë°©ì‹ìœ¼ë¡œ ë‹¤ìŒì„ ì‹¤í–‰í•˜ì—¬, íŒŒì¸íŠœë‹í•œ ëª¨ë¸ differenceë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
```sh
CUDA_VISIBLE_DEVICES=0 \
    deepspeed examples/evaluate.py \
    --answer_type text \
    --model_name_or_path ${llama-hf-path}/llama-7b-hf \
    --lora_model_path output_models/${llama-model-diff-path} \
    --dataset_path data/alpaca/test \
    --prompt_structure "Input: {input}" \
    --deepspeed examples/ds_config.json
```
ì´ì œ íŒŒì¸íŠœë‹ëœ llama ëª¨ë¸ë¡œ í‰ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### 4.2 DeepSpeed Config
configëŠ” configsë¥¼ í†µí•´ êµ¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [DeepSpeed Configuration](https://www.deepspeed.ai/docs/config-json/)ì°¸ì¡°í•˜ì‹­ì‹œì˜¤.

## 5. Model Release

### 5.1 Medical Model Checkpoints
ë‹¤ìŒ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ì—¬ ì €í¬ì˜ ì˜ë£Œ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë‹¤ìš´ë¡œë“œ í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```bash
cd output_models
bash download.sh medical_ckpt
cd -
```
ë˜í•œ ë‹¤ìŒ Google ë“œë¼ì´ë¸Œ ë§í¬ë¥¼ í†µí•´ ì§ì ‘ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤ : [medical_ckpt.tar.gz](https://drive.google.com/file/d/1bnsQGNGNYchsOfiNyRAmL2fNiowbmFNw/view?usp=share_link)

### 5.2 Instruction Model Checkpoints
ë‹¤ìŒ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ì—¬ ì €í¬ì˜ instruction ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë‹¤ìš´ë¡œë“œ í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
```bash
cd output_models
bash download.sh instruction_ckpt
cd -
```

ë‹¤ìŒ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ì—¬ ì €í¬ì˜ instruction ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë‹¤ìš´ë¡œë“œ í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. [instruction_ckpt.tar.gz](https://drive.google.com/file/d/1d_ioQ-ViVweeifbsFSO4pczc3UORFHZO/view?usp=share_link)

### 5.3 Begin Reproduce

ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë‹¤ìš´ë¡œë“œ í•œ í›„ì—ëŠ” `--lora_model_path` ë¥¼ `output_models/instruction_ckpt/llama7b-lora`(llama-7b for instructionì˜ ì˜ˆì‹œ)ë¡œ ëŒ€ì²´í•˜ê³ , `--model_name_or_path` ë¥¼ `LMFlow/scripts/run_evaluation_with_lora.sh` ë‚´ë¶€ì˜ ë³€í™˜ ëœ llama ëª¨ë¸ë¡œ ëŒ€ì²´ í•œ ë‹¤ìŒ ì´ ì…¸ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ì—¬ ê²°ê³¼ë¥¼ ì¬í˜„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ê·¸ëŸ° ë‹¤ìŒ [ë¬¸ì„œ](https://optimalscale.github.io/LMFlow/)ì—ì„œ ëª¨ë¸ ì„±ëŠ¥ì„ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## Documentation
ë” ë§ì€ API ì°¸ì¡° ë° ì‹¤í—˜ ê²°ê³¼ëŠ” [Documentation](https://optimalscale.github.io/LMFlow/)ë¥¼ ì°¸ì¡°í•˜ì‹­ì‹œì˜¤.

## Vision
ì•ˆë…•í•˜ì„¸ìš”! LMflowëŠ” ì™„ì „í•œ LLM í•™ìŠµ í”„ë¡œì„¸ìŠ¤ë¥¼ í¬í•¨í•˜ì—¬ ì‚¬ìš©ìê°€ ìì‹ ì˜ ì–¸ì–´ ëª¨ë¸ì„ ë¹ ë¥´ê²Œ êµ¬ì¶•í•˜ê³  íš¨ê³¼ì ìœ¼ë¡œ í•™ìŠµ í•  ìˆ˜ ìˆë„ë¡í•˜ëŠ” ì½”ë“œ repositoryê°€ ê³§ ì¶œì‹œ ë  ê²ƒì„ ë°œí‘œí•˜ê²Œ ë˜ì–´ ê¸°ì©ë‹ˆë‹¤.

ìš°ë¦¬ì˜ ì½”ë“œ repositoryëŠ” ë‹¨ìˆœí•œ ëª¨ë¸ì´ ì•„ë‹ˆë©° ì™„ì „í•œ í•™ìŠµ ì›Œí¬ í”Œë¡œ, ëª¨ë¸ ìµœì í™” ë° í…ŒìŠ¤íŠ¸ ë„êµ¬ë¥¼ í¬í•¨í•©ë‹ˆë‹¤. ëŒ€í™” ëª¨ë¸, ì§ˆë¬¸/ë‹µë³€ ëª¨ë¸ ë° ê¸°íƒ€ í…ìŠ¤íŠ¸ ìƒì„± ëª¨ë¸ì„ ë¹„ë¡¯í•œ ë‹¤ì–‘í•œ ìœ í˜•ì˜ ì–¸ì–´ ëª¨ë¸ì„ êµ¬ì¶•í•˜ëŠ” ë° ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ë˜í•œ LMflowëŠ” LLM ê³µìœ  í”Œë«í¼ì„ ë§Œë“¤ì–´ ì‚¬ëŒë“¤ì´ ì²´í¬í¬ì¸íŠ¸ì™€ ê²½í—˜ì„ ê³µìœ í•˜ì—¬ ì»¤ë®¤ë‹ˆí‹°ì˜ ê¸°ìˆ ì„ í•¨ê»˜ ê°œì„ í•  ìˆ˜ ìˆëŠ” ê°œë°©ì ì´ê³  ë¯¼ì£¼ì ì¸ LLM ê³µìœ  í”Œë«í¼ì„ ë§Œë“¤ê³ ìí•©ë‹ˆë‹¤. LLMì— ê´€ì‹¬ìˆëŠ” ëˆ„êµ¬ë‚˜ ì°¸ì—¬í•˜ì—¬ ì¹œê·¼í•˜ê³  ê°œë°©ì ì¸ ì»¤ë®¤ë‹ˆí‹°ë¥¼ ë§Œë“¤ì–´ ê°€ê³ ì í•©ë‹ˆë‹¤.

ì´ˆë³´ìë“  ì „ë¬¸ê°€ë“  ìƒê´€ì—†ì´ ì´ í”Œë«í¼ì—ì„œ ë§ì€ í˜œíƒì„ ë°›ì„ ìˆ˜ ìˆì„ ê²ƒì´ë¼ê³  ë¯¿ìŠµë‹ˆë‹¤. í•¨ê»˜ í™œê¸°ì°¨ê³  í˜ì‹ ì ì¸ LLM ì»¤ë®¤ë‹ˆí‹°ë¥¼ ë§Œë“¤ì–´ ë´…ì‹œë‹¤!

[![Embark](https://img.shields.io/badge/discord-LMFlow-%237289da.svg?logo=discord)](https://discord.gg/u9VJNpzhvA)
[![slack badge](https://img.shields.io/badge/Slack-join-blueviolet?logo=slack&amp)](https://join.slack.com/t/lmflow/shared_invite/zt-1wju9nicy-woXbNtS~5MavHSAtiMxmxQ)
[![WeChat badge](https://img.shields.io/badge/WeChat-Join-brightgreen?logo=wechat&amp)](https://i.328888.xyz/2023/04/04/ibvpAk.jpeg)

## Disclaimer

ì´ íŒ¨í‚¤ì§€ëŠ” ëŒ€í˜• ëª¨ë¸ íŠœë‹ì„ ìœ„í•œ ê°„ì†Œí™” ëœ ì‚¬ìš©ì ì¹œí™”ì ì¸ íŒŒì´í”„ ë¼ì¸ì„ ì œê³µí•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œí•©ë‹ˆë‹¤. ë”°ë¼ì„œ ì–´ë– í•œ ë²•ì ì¸ ì±…ì„ë„ ì§€ì§€ ì•ŠìŠµë‹ˆë‹¤.
ê·¸ ê¸°ëŠ¥ì€ ì°¸ì¡° ìš©ë„ë¡œ ì œê³µë˜ë©° ì‚¬ìš©ìê°€ ì‚¬ìš©í•˜ë„ë¡ ì˜ë„ë˜ì—ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ë°ì´í„° ë° ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ê³¼ ê´€ë ¨ëœ ì±…ì„ì€ ì‚¬ìš©ìì—ê²Œ ë‹¬ë ¤ìˆìŠµë‹ˆë‹¤. ì´ íŒ¨í‚¤ì§€ëŠ” ì‚¬ìš©ì ì¤€ë¹„ êµ¬ì„± ìš”ì†Œì˜ ì •í™•ì„±, ì™„ì „ì„±, ì ìš© ê°€ëŠ¥ì„± ë˜ëŠ” ë²•ì  ì í•©ì„±ì„ ë³´ì¦í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì‚¬ìš©ìëŠ” ëª¨ë¸ ë° ë°ì´í„°ì˜ ì¤€ë¹„ì™€ ê´€ë ¨ëœ ëª¨ë“  ìœ„í—˜ê³¼ ì±…ì„ì„ ì¸ì‹í•˜ê³  ê°€ì •í•˜ê³  ì´ íŒ¨í‚¤ì§€ë¥¼ í™œìš©í•˜ê¸° ì „ì— ë²•ì , ìƒì—…ì  ë° ê¸°ìˆ ì  ìë¬¸ì„ ë°›ì•„ì•¼ë§Œ í•©ë‹ˆë‹¤. íŒŒì´í”„ ë¼ì¸ì€ ì‚¬ìš©ìì˜ ì˜ëª»ëœ ë°ì´í„° ë° ì‚¬ì „ í•™ìŠµ ëœ ëª¨ë¸ì˜ ì¤€ë¹„ë¡œ ì¸í•œ ì–´ë– í•œ ì§ì ‘ì ì¸, ê°„ì ‘ì ì¸, íŠ¹ìˆ˜, ë¶€ìˆ˜ì  ë˜ëŠ” ê²°ê³¼ì  ì†í•´ì— ëŒ€í•´ì„œë„ ì±…ì„ì„ ì§€ì§€ ì•ŠìŠµë‹ˆë‹¤.

ì˜ì–´ì™€ ì¤‘êµ­ì–´ ë²„ì „ ëª¨ë‘ë¥¼ í¬í•¨í•˜ëŠ” ì ê²€ í¬ì¸íŠ¸ëŠ” ì—°êµ¬ ëª©ì ìœ¼ë¡œë§Œ ì œê³µë©ë‹ˆë‹¤. 
ì´ëŸ¬í•œ ì²´í¬ í¬ì¸íŠ¸ì— í¬í•¨ ëœ êµìœ¡ ë°ì´í„°ì—ëŠ” ChatGPT ì–¸ì–´ ëª¨ë¸ì—ì„œ ìƒì„± ëœ ê²°ê³¼ê°€ í¬í•¨ë©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì²´í¬ í¬ì¸íŠ¸ì˜ ë°°í¬ ë˜ëŠ” ì‚¬ìš©ì„ ë³´ì¦í•˜ê±°ë‚˜ ì¥ë ¤í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ì²´í¬ í¬ì¸íŠ¸ì˜ ì‚¬ìš©ìëŠ” ì˜¬ë°”ë¥´ê³  ì ì ˆí•˜ê²Œ ì‚¬ìš©ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ëŠ” ê²ƒì€ ì „ì ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì±…ì„ì…ë‹ˆë‹¤.

ë˜í•œ ëª¨ë¸ì—ì„œ ìƒì„± ëœ ê²°ê³¼ëŠ” í™•ë¥  ëª¨ë¸ì— ê¸°ë°˜í•˜ë©° ì§ì ‘ì ìœ¼ë¡œ ì´ íŒŒì´í”„ ë¼ì¸ê³¼ ê´€ë ¨ì´ ì—†ìŒì„ ê°•ì¡°í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ê²°ê³¼ì˜ ì •í™•ì„±, ì‹ ë¢°ì„±, ì ìš© ê°€ëŠ¥ì„± ë° ë²•ì  ì í•©ì„±ì€ ì´ íŒŒì´í”„ ë¼ì¸ì—ì„œ ë³´ì¦ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ì‚¬ìš©ìëŠ” ê²°ê³¼ì™€ ê´€ë ¨ëœ ìœ„í—˜ê³¼ ì±…ì„ë„ ì¸ì‹í•´ì•¼í•˜ë©° ëª¨ë¸ì—ì„œ ìƒì„± ëœ ê²°ê³¼ì— ì˜ì¡´í•˜ê¸° ì „ì— ë²•ì , ìƒì—…ì  ë° ê¸°ìˆ ì  ìë¬¸ì„ ë°›ì•„ì•¼í•©ë‹ˆë‹¤. íŒŒì´í”„ ë¼ì¸ì€ ì‚¬ìš©ìê°€ ëª¨ë¸ì—ì„œ ìƒì„± í•œ ê²°ê³¼ì— ì˜ì¡´í•˜ì—¬ ë°œìƒí•˜ëŠ” ì–´ë– í•œ ì§ì ‘ì ì¸, ê°„ì ‘ì ì¸, íŠ¹ìˆ˜, ë¶€ìˆ˜ì  ë˜ëŠ” ê²°ê³¼ì  ì†í•´ì— ëŒ€í•´ì„œë„ ì±…ì„ì§€ì§€ ì•ŠìŠµë‹ˆë‹¤.

## Support

ë„ì›€ì´ í•„ìš”í•˜ë©´ ê³µì‹ [ê¹ƒ í—ˆë¸Œ ë ˆí¬ì§€í† ë¦¬](https://github.com/OptimalScale/LMFlow)ì— ì´ìŠˆë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.

## Contributors
<a href="https://github.com/OptimalScale/LMFlow/graphs/contributors">
  <img src="https://contrib.rocks/image?repo=OptimalScale/LMFlow" />
</a>

## Citation
ì´ repositoryë¥¼ ìœ ìš©í•˜ê²Œ ì‚¬ìš©í•˜ì…¨ë‹¤ë©´ â­ì„ ëˆŒëŸ¬ì£¼ì‹œê³  ë‹¤ìŒì„ í†µí•´ ì¸ìš©í•´ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤.

```bibtex
@misc{lmflow,
  author = {Shizhe Diao and Rui Pan and Hanze Dong and KaShun Shum and Jipeng Zhang and Wei Xiong and Tong Zhang},
  title = {LMFlow: An Extensible Toolkit for Finetuning and Inference of Large Foundation Models},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://optimalscale.github.io/LMFlow/}},
}
```
