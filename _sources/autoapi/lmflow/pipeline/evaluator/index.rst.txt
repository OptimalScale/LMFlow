lmflow.pipeline.evaluator
=========================

.. py:module:: lmflow.pipeline.evaluator

.. autoapi-nested-parse::

   The Evaluator class simplifies the process of running evaluation on a language model provided
   by a HFDecoderModel instance imported from the lmflow package. The class constructor takes three
   dictionaries as arguments: model_args containing arguments related to the language model,
   data_args containing arguments related to the data used for evaluation, and evaluator_args
   containing other arguments for the evaluation process.

   The class has two methods: create_dataloader() that loads the data from the test file, creates
   a data loader, and returns it with the size of the data, and evaluate(model) that generates
   output text given input text. It uses the create_dataloader() method to load the data, iterates
   over the data in mini-batches, and encodes the input text with the encode() method of the
   HFDecoderModel class. Then, it generates output text using the evaluate() method of the HFDecoderModel
   class, decodes the generated output text using the decode() method of the HFDecoderModel class, and
   writes the output to a file in the output directory. The method also logs some information to the
   console and Weights and Biases if the use_wandb argument is True.

   ..
       !! processed by numpydoc !!


Classes
-------

.. autoapisummary::

   lmflow.pipeline.evaluator.Evaluator


Module Contents
---------------

.. py:class:: Evaluator(model_args: lmflow.args.ModelArguments, data_args: lmflow.args.DatasetArguments, evaluator_args: lmflow.args.EvaluatorArguments)

   Bases: :py:obj:`lmflow.pipeline.base_pipeline.BasePipeline`


   
   Initializes the `Evaluator` class with given arguments.


   :Parameters:

       **model_args** : ModelArguments object.
           Contains the arguments required to load the model.

       **data_args** : DatasetArguments object.
           Contains the arguments required to load the dataset.

       **evaluator_args** : EvaluatorArguments object.
           Contains the arguments required to perform evaluation.














   ..
       !! processed by numpydoc !!

   .. py:attribute:: data_args


   .. py:attribute:: evaluator_args


   .. py:attribute:: model_args


   .. py:attribute:: local_rank


   .. py:attribute:: world_size


   .. py:attribute:: config


   .. py:attribute:: block_size


   .. py:method:: create_dataloader(dataset: lmflow.datasets.dataset.Dataset)


   .. py:method:: _match(predicted_answer, groundtruth, answer_type=None)


   .. py:method:: evaluate(model, dataset: lmflow.datasets.dataset.Dataset, metric='accuracy', verbose=True)

      
      Perform Evaluation for a model


      :Parameters:

          **model** : TunableModel object.
              TunableModel to perform inference

          **dataset** : Dataset object.
              ..














      ..
          !! processed by numpydoc !!


   .. py:method:: _evaluate_acc_with_accelerate(model, dataset, verbose=True)


   .. py:method:: _evaluate_acc_with_deepspeed(model, dataset, verbose=True)


   .. py:method:: _evaluate_ppl(model, dataset: lmflow.datasets.dataset.Dataset, verbose=True)


   .. py:method:: _evaluate_nll(model, dataset: lmflow.datasets.dataset.Dataset, verbose=True)

      
      Evaluates negative log likelihood of the model over a dataset.

      NLL = -1/N sum_{i=1}^N sum_{j=1}^|w_i| ln(p(w_{i,j}|context_window)),

      where N is the number of data samples, w_{i,j} is the j-th token in
      i-th sample. Here "context_window" = p(w_{i,start}, w_{i,start+1}, ...,
      p_{i,j-1} with start = max(0, j - window_length + 1). "window_length"
      is normally the maximum length accepted by the model.

      Returns:
          A float which represents the negative log likelihood.















      ..
          !! processed by numpydoc !!


