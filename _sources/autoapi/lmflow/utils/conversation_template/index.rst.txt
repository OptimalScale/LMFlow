lmflow.utils.conversation_template
==================================

.. py:module:: lmflow.utils.conversation_template


Submodules
----------

.. toctree::
   :maxdepth: 1

   /autoapi/lmflow/utils/conversation_template/base/index
   /autoapi/lmflow/utils/conversation_template/chatglm/index
   /autoapi/lmflow/utils/conversation_template/chatml/index
   /autoapi/lmflow/utils/conversation_template/deepseek/index
   /autoapi/lmflow/utils/conversation_template/gemma/index
   /autoapi/lmflow/utils/conversation_template/hymba/index
   /autoapi/lmflow/utils/conversation_template/internlm/index
   /autoapi/lmflow/utils/conversation_template/llama/index
   /autoapi/lmflow/utils/conversation_template/phi/index
   /autoapi/lmflow/utils/conversation_template/qwen/index
   /autoapi/lmflow/utils/conversation_template/yi/index
   /autoapi/lmflow/utils/conversation_template/zephyr/index


Attributes
----------

.. autoapisummary::

   lmflow.utils.conversation_template.EMPTY_TEMPLATE
   lmflow.utils.conversation_template.EMPTY_NO_SPECIAL_TOKENS_TEMPLATE
   lmflow.utils.conversation_template.CHATGLM3_TEMPLATE
   lmflow.utils.conversation_template.CHATML_TEMPLATE
   lmflow.utils.conversation_template.DEEPSEEK_V2_TEMPLATE
   lmflow.utils.conversation_template.DEEPSEEK_V3_TEMPLATE
   lmflow.utils.conversation_template.DEEPSEEK_R1_TEMPLATE
   lmflow.utils.conversation_template.DEEPSEEK_R1_DISTILL_TEMPLATE
   lmflow.utils.conversation_template.GEMMA_TEMPLATE
   lmflow.utils.conversation_template.HYMBA_TEMPLATE
   lmflow.utils.conversation_template.INTERNLM2_TEMPLATE
   lmflow.utils.conversation_template.LLAMA2_TEMPLATE
   lmflow.utils.conversation_template.LLAMA3_TEMPLATE
   lmflow.utils.conversation_template.LLAMA3_TEMPLATE_FOR_TOOL
   lmflow.utils.conversation_template.PHI3_TEMPLATE
   lmflow.utils.conversation_template.QWEN2_TEMPLATE
   lmflow.utils.conversation_template.QWEN2_TEMPLATE_FOR_TOOL
   lmflow.utils.conversation_template.QWEN2_5_TEMPLATE
   lmflow.utils.conversation_template.QWEN2_5_1M_TEMPLATE
   lmflow.utils.conversation_template.QWEN2_5_MATH_TEMPLATE
   lmflow.utils.conversation_template.QWEN_QWQ_TEMPLATE
   lmflow.utils.conversation_template.YI1_5_TEMPLATE
   lmflow.utils.conversation_template.ZEPHYR_TEMPLATE
   lmflow.utils.conversation_template.logger
   lmflow.utils.conversation_template.PRESET_TEMPLATES
   lmflow.utils.conversation_template.JINJA_TEMPLATES


Classes
-------

.. autoapisummary::

   lmflow.utils.conversation_template.ConversationTemplate
   lmflow.utils.conversation_template.ConversationTemplateForTool


Functions
---------

.. autoapisummary::

   lmflow.utils.conversation_template.is_package_version_at_least


Package Contents
----------------

.. py:function:: is_package_version_at_least(package_name, min_version)

.. py:data:: EMPTY_TEMPLATE

.. py:data:: EMPTY_NO_SPECIAL_TOKENS_TEMPLATE

.. py:class:: ConversationTemplate

   .. py:attribute:: user_formatter
      :type:  Formatter


   .. py:attribute:: assistant_formatter
      :type:  Formatter


   .. py:attribute:: function_formatter
      :type:  Optional[Formatter]
      :value: None



   .. py:attribute:: observation_formatter
      :type:  Optional[Formatter]
      :value: None



   .. py:attribute:: system_formatter
      :type:  Optional[Formatter]
      :value: None



   .. py:attribute:: force_system
      :type:  bool
      :value: False



   .. py:attribute:: tools_formatter
      :type:  Optional[Formatter]
      :value: None



   .. py:attribute:: separator
      :type:  Optional[TemplateComponent]
      :value: None



   .. py:attribute:: remove_last_sep
      :type:  bool
      :value: False



   .. py:attribute:: special_starter
      :type:  Optional[TemplateComponent]
      :value: None



   .. py:attribute:: special_stopper
      :type:  Optional[TemplateComponent]
      :value: None



   .. py:attribute:: template_name
      :type:  Optional[str]
      :value: None



   .. py:attribute:: system_default
      :type:  Optional[str]
      :value: None



   .. py:method:: __post_init__()


   .. py:method:: encode_conversation(tokenizer: transformers.PreTrainedTokenizer, messages: List[Dict[str, str]], system: Optional[str] = None, tools: Optional[List[str]] = None, **kwargs) -> Sequence[Tuple[List[int], List[int]]]

      
      Messages here should be guaranteed to be in pairs, with the first message being the user message and the second message being the system message.
      Data example: 
      ```json
      {
          "conversation_id": 2,
          "system": "sysinfo1",
          "tools": ["tool_1_desc"],
          "messages": [
              {
                  "role": "user",
                  "content": "hi"
              },
              {
                  "role": "assistant",
                  "content": "Hello!"
              }
          ]
      }
      ```
















      ..
          !! processed by numpydoc !!


   .. py:method:: _encode(tokenizer: transformers.PreTrainedTokenizer, messages: List[Dict[str, str]], system: Optional[str] = None, tools: Optional[str] = None, **kwargs) -> Sequence[Tuple[List[int], List[int]]]


   .. py:method:: _encode_template(template: List[TemplateComponent], tokenizer: transformers.PreTrainedTokenizer, **kwargs) -> List[int]

      
      Encode template components into token ids.


      :Parameters:

          **template** : List[TemplateComponent]
              Formatted template components.

          **tokenizer** : PreTrainedTokenizer
              Tokenizer to convert tokens into token ids.



      :Returns:

          List[int]
              Encoded token ids.











      ..
          !! processed by numpydoc !!


   .. py:method:: post_process_pairs(encoded_pairs, tokenizer)


   .. py:method:: remove_last_separator(encoded_pairs: Sequence[Tuple[List[int], List[int]]], tokenizer: transformers.PreTrainedTokenizer) -> Sequence[Tuple[List[int], List[int]]]


   .. py:method:: add_special_starter(encoded_pairs: Sequence[Tuple[List[int], List[int]]], tokenizer: transformers.PreTrainedTokenizer) -> Sequence[Tuple[List[int], List[int]]]


   .. py:method:: add_special_stopper(encoded_pairs: Sequence[Tuple[List[int], List[int]]], tokenizer: transformers.PreTrainedTokenizer) -> Sequence[Tuple[List[int], List[int]]]


   .. py:method:: _ensure_id_list(obj: Union[int, List[int]]) -> List[int]

      
      Make sure the object is a list of integers. Useful for handling token ids.
















      ..
          !! processed by numpydoc !!


.. py:class:: ConversationTemplateForTool

   Bases: :py:obj:`ConversationTemplate`


   .. py:method:: encode_conversation(tokenizer: transformers.PreTrainedTokenizer, messages: List[Dict[str, str]], system: Optional[str] = None, tools: Optional[List[str]] = None, **kwargs) -> Sequence[Tuple[List[int], List[int]]]

      
      Messages here should be guaranteed to be in pairs, with the first message being the user message and the second message being the system message.
      Data example: 
      ```json
      {
          "conversation_id": 2,
          "system": "sysinfo1",
          "tools": ["tool_1_desc"],
          "messages": [
              {
                  "role": "user",
                  "content": "hi"
              },
              {
                  "role": "assistant",
                  "content": "Hello!"
              }
          ]
      }
      ```
















      ..
          !! processed by numpydoc !!


   .. py:method:: _encode(tokenizer: transformers.PreTrainedTokenizer, messages: List[Dict[str, str]], system: Optional[str] = None, tools: Optional[str] = None, **kwargs) -> Sequence[Tuple[List[int], List[int]]]


   .. py:method:: _encode_template(template: List[TemplateComponent], tokenizer: transformers.PreTrainedTokenizer, **kwargs) -> List[int]

      
      Encode template components into token ids.


      :Parameters:

          **template** : List[TemplateComponent]
              Formatted template components.

          **tokenizer** : PreTrainedTokenizer
              Tokenizer to convert tokens into token ids.



      :Returns:

          List[int]
              Encoded token ids.











      ..
          !! processed by numpydoc !!


   .. py:method:: _handle_tools(tools: Optional[List[str]]) -> str


.. py:data:: CHATGLM3_TEMPLATE

.. py:data:: CHATML_TEMPLATE

.. py:data:: DEEPSEEK_V2_TEMPLATE

.. py:data:: DEEPSEEK_V3_TEMPLATE
   :value: Multiline-String

   .. raw:: html

      <details><summary>Show Value</summary>

   .. code-block:: python

      """{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='', is_first_sp=true) %}{%- for message in messages %}{%- if message['role'] == 'system' %}{%- if ns.is_first_sp %}{% set ns.system_prompt = ns.system_prompt + message['content'] %}{% set ns.is_first_sp = false %}{%- else %}{% set ns.system_prompt = ns.system_prompt + '
      
      ' + message['content'] %}{%- endif %}{%- endif %}{%- endfor %}{{bos_token}}{{ns.system_prompt}}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{{'<｜User｜>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is none %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls']%}{%- if not ns.is_first %}{{'<｜Assistant｜>'}}{% generation %}{{'<｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '
      ' + '```json' + '
      ' + tool['function']['arguments'] + '
      ' + '```' + '<｜tool▁call▁end｜>'}}{% endgeneration %}{%- set ns.is_first = true -%}{%- else %}{% generation %}{{'
      ' + '<｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '
      ' + '```json' + '
      ' + tool['function']['arguments'] + '
      ' + '```' + '<｜tool▁call▁end｜>'}}{{'<｜tool▁calls▁end｜><｜end▁of▁sentence｜>'}}{% endgeneration %}{%- endif %}{%- endfor %}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is not none %}{%- if ns.is_tool %}{{'<｜tool▁outputs▁end｜>'}}{% generation %}{{ message['content'] + '<｜end▁of▁sentence｜>'}}{%- set ns.is_tool = false -%}{% endgeneration %}{%- else %}{{'<｜Assistant｜>'}}{% generation %}{{ message['content'] + '<｜end▁of▁sentence｜>'}}{% endgeneration %}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<｜tool▁outputs▁begin｜><｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- set ns.is_output_first = false %}{%- else %}{{'
      <｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<｜tool▁outputs▁end｜>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<｜Assistant｜>'}}{% endif %}"""

   .. raw:: html

      </details>



.. py:data:: DEEPSEEK_R1_TEMPLATE
   :value: "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif...


.. py:data:: DEEPSEEK_R1_DISTILL_TEMPLATE
   :value: "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif...


.. py:data:: GEMMA_TEMPLATE

.. py:data:: HYMBA_TEMPLATE

.. py:data:: INTERNLM2_TEMPLATE

.. py:data:: LLAMA2_TEMPLATE

.. py:data:: LLAMA3_TEMPLATE

.. py:data:: LLAMA3_TEMPLATE_FOR_TOOL

.. py:data:: PHI3_TEMPLATE

.. py:data:: QWEN2_TEMPLATE

.. py:data:: QWEN2_TEMPLATE_FOR_TOOL

.. py:data:: QWEN2_5_TEMPLATE
   :value: '{%- if tools %}{{- \'<|im_start|>system\\n\' }}{%- if messages[0][\'role\'] == \'system\' %}{{-...


.. py:data:: QWEN2_5_1M_TEMPLATE
   :value: '{%- if tools %}{{- \'<|im_start|>system\\n\' }}{%- if messages[0][\'role\'] == \'system\' %}{{-...


.. py:data:: QWEN2_5_MATH_TEMPLATE
   :value: '{%- if tools %}{{- \'<|im_start|>system\\n\' }}{%- if messages[0][\'role\'] == \'system\' %}{{-...


.. py:data:: QWEN_QWQ_TEMPLATE
   :value: '{%- if tools %}{{- \'<|im_start|>system\\n\' }}{%- if messages[0][\'role\'] == \'system\' %}{{-...


.. py:data:: YI1_5_TEMPLATE

.. py:data:: ZEPHYR_TEMPLATE

.. py:data:: logger

.. py:data:: PRESET_TEMPLATES

.. py:data:: JINJA_TEMPLATES

