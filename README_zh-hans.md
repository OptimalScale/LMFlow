<p align="center" width="100%">
<img src="assets/logo.png" alt="LMFlow" style="width: 100%; min-width: 300px; display: block; margin: auto; background-color: transparent;">
</p>

# LMFlow

<h4 align="center">
    <p>
        <a href="https://github.com/OptimalScale/LMFlow/blob/main/README.md">English</a> |
        <b>ç®€ä½“ä¸­æ–‡</b> |
        <a href="https://github.com/OptimalScale/LMFlow/blob/main/README_es.md">EspaÃ±ol</a> |
        <a href="https://github.com/OptimalScale/LMFlow/blob/main/README_jp.md">æ—¥æœ¬èª</a>
    <p>
</h4>

[![Code License](https://img.shields.io/badge/Code%20License-Apache_2.0-green.svg)](https://github.com/OptimalScale/LMFlow/blob/main/LICENSE)
[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/release/python-390/)
[![Doc](https://img.shields.io/badge/Website-Doc-ff69b4.svg)](https://optimalscale.github.io/LMFlow/)
[![Embark](https://img.shields.io/badge/discord-LMFlow-%237289da.svg?logo=discord)](https://discord.gg/srGxyazbNs)
[![WeChat badge](https://img.shields.io/badge/å¾®ä¿¡-åŠ å…¥-brightgreen?logo=wechat&amp)](https://i.328888.xyz/2023/04/04/ibvpAk.jpeg)
[![slack badge](https://img.shields.io/badge/Slack-join-blueviolet?logo=slack&amp)](https://join.slack.com/t/lmflow/shared_invite/zt-1s6egx12s-THlwHuCjF6~JGKmx7JoJPA)

ä¸€ä¸ªå¯æ‰©å±•ã€æ–¹ä¾¿å’Œé«˜æ•ˆçš„å·¥å…·ç®±ï¼Œç”¨äºå¾®è°ƒå¤§å‹æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯å¼€å‘ä¸€å¥—ç”¨æˆ·å‹å¥½ã€å¿«é€Ÿå¯é ï¼Œå¹¶å¯¹æ•´ä¸ªç¤¾åŒºå¼€æ”¾çš„å…¨æµç¨‹å¾®è°ƒä»£ç åº“ã€‚

æ‰€æœ‰äººçš„å¤§è¯­è¨€æ¨¡å‹ã€‚è¯·æŸ¥çœ‹æˆ‘ä»¬çš„[æ„¿æ™¯](https://github.com/OptimalScale/LMFlow#vision)


<p align="center" width="100%">
<img src="assets/features.png" alt="LMFlow-features" style="width: 100%; min-width: 300px; display: block; margin: auto;">
</p>


## Latest News
* [2023-04-02] [Web service is online!](https://lmflow.com/)
* [2023-04-01] [Release Chinese checkpoints in model zoo: LLaMA-7B-tuned, LLaMA-13B-tuned, LLaMA-33B-tuned.](https://github.com/OptimalScale/LMFlow#model-zoo)
* [2023-04-01] [Release English checkpoints in model zoo: LLaMA-7B-medical, LLaMA-13B-medical, and LLaMA-33B-medical.](https://github.com/OptimalScale/LMFlow#model-zoo)
* [2023-03-27] [Support full tuning and lora tuning for all decoder models.](https://github.com/OptimalScale/LMFlow#supported-models) 
* [2023-03-27] [Tasked tuned model beats ChatGPT on medical domain](https://github.com/OptimalScale/LMFlow#model-performance)
* [2023-03-27] [Release code and checkpoints - version 0.0.1](https://optimalscale.github.io/LMFlow/)

## Demos

### å½“å‰æˆ‘ä»¬çš„æ£€æŸ¥ç‚¹ä¸‹è½½æœåŠ¡å·²æ»¡è´Ÿè·ã€‚æˆ‘ä»¬å¢åŠ äº†ä¸€ä¸ªæœåŠ¡å™¨æ¥æ”¯æŒè¯¥æœåŠ¡ã€‚å¦‚æœæ‚¨é‡åˆ°"_too many HTTP requests_"çš„é”™è¯¯ï¼Œè¯·ç­‰å¾…å‡ åˆ†é’Ÿåå†è¯•ã€‚è°¢è°¢æ‚¨çš„ç†è§£ã€‚

æˆ‘ä»¬æä¾›å››ç§æ¼”ç¤ºï¼ŒåŒ…æ‹¬ï¼š
- åœ¨çº¿æœåŠ¡ï¼šå¦‚æœæ‚¨ä¸æƒ³è¿è¡Œä»»ä½•ä»£ç ï¼Œåªæ˜¯æƒ³å°è¯•æˆ‘ä»¬çš„æ¨¡å‹ï¼Œæˆ‘ä»¬éƒ¨ç½²äº†è°ƒæ•´æŒ‡ä»¤çš„LLaMA-7Bå’ŒLLaMA-33Bä¾›æ‚¨å°è¯•ã€‚
- Colab Chatbot (shell)ï¼šä¸€ä¸ªåŸºäºäº¤äº’å¼shellçš„èŠå¤©æœºå™¨äººï¼Œè®©æ‚¨å¯ä»¥è½»æ¾åœ¨Colabä¸Šéƒ¨ç½²èŠå¤©æœºå™¨äººã€‚
- Colab Chatbot (web)ï¼šä¸€ä¸ªåŸºäºäº¤äº’å¼Webçš„èŠå¤©æœºå™¨äººï¼Œè®©æ‚¨å¯ä»¥è½»æ¾åœ¨Colabä¸Šéƒ¨ç½²è‡ªå·±çš„èŠå¤©æœºå™¨äººã€‚
- æœ¬åœ°éƒ¨ç½²ï¼šæˆ‘ä»¬è¿˜æä¾›ä¸€ç§æ–¹å¼ï¼Œè®©æ‚¨å¯ä»¥åœ¨æœ¬åœ°éƒ¨ç½²æ¨¡å‹/èŠå¤©æœºå™¨äººï¼Œè¿™æ„å‘³ç€å¦‚æœæ‚¨æœ‰è¶³å¤Ÿçš„èµ„æºï¼Œæ‚¨å¯ä»¥éƒ¨ç½²æ¯”å‰ä¸‰ç§æ–¹æ³•æ›´å¤§çš„æ¨¡å‹ã€‚


[![Code License](https://img.shields.io/badge/Online%20Service-Web-green.svg)](https://lmflow.com)
[![colab badge](https://img.shields.io/badge/Colab-(shell)%20%20chatbot:%20gpt--neo-orange?logo=google-colab&amp)](https://colab.research.google.com/drive/1P9Hf6_mLE7WHH92pw73j9D5kz6GTdkow?usp=sharing)
[![colab badge](https://img.shields.io/badge/Colab-(web)%20%20chatbot:%20gpt--neo-blue?logo=google-colab&amp)](https://colab.research.google.com/drive/1LLtiiQO-ZIIFsTKxYzGWYX9BDRc-v8dq?usp=sharing)



### Online Service
>æ¬¢è¿è®¿é—®æˆ‘ä»¬çš„[åœ¨çº¿æ¼”ç¤º](https://lmflow.com/)ã€‚æˆ‘ä»¬éƒ¨ç½²äº†ç»è¿‡LLaMA-7Bå’ŒLLaMA-33Bè°ƒæ•´æŒ‡ä»¤çš„æ¨¡å‹è¿›è¡Œé¢„è§ˆã€‚ç”±äºç½‘ç«™è®¿é—®é‡è¾ƒé«˜ï¼Œæœ‰æ—¶ç½‘ç«™å¯èƒ½æ— æ³•å“åº”ã€‚æ‚¨ä¹Ÿå¯ä»¥å‚è€ƒâ€œæœ¬åœ°éƒ¨ç½²â€æ¥éƒ¨ç½²è‡ªå·±çš„èŠå¤©æœºå™¨äººã€‚

### Colab chatbot(shell)
<p align="center" width="100%">
<img src="assets/colab-shell-chatbot-demo.png">
</p>


æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªä½¿ç”¨Google Colabçš„T4/P100/V100 GPUçš„èŠå¤©æœºå™¨äººç®€å•shellæ¼”ç¤ºã€‚è¯·æ³¨æ„ï¼Œæä¾›çš„gpt-neo-2.7bæ¨¡å‹æ˜¯ç›¸å¯¹è¾ƒå¼±çš„æ¨¡å‹ï¼Œä»…æ”¯æŒè‹±æ–‡ï¼Œå¹¶ä¸”æœ‰æ—¶ä¼šç”Ÿæˆä¸ç†æƒ³çš„å“åº”ã€‚ä¸ºäº†æ”¹å–„æ€§èƒ½ï¼Œç”¨æˆ·å¯ä»¥ä½¿ç”¨è‡ªå·±çš„æ•°æ®é›†è¿›è¡Œå¾®è°ƒï¼Œå¹¶ä½¿ç”¨LMFlowè·å¾—æ›´å¥½çš„æ¨¡å‹ã€‚ä¹Ÿå¯ä»¥å°è¯•å…¶ä»–å¯ç”¨çš„ä»…è§£ç å™¨ï¼ˆdecoder onlyï¼‰æ¨¡å‹ã€‚
ğŸ¤— [huggingface](https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads), by

```sh
./scripts/run_chatbot.sh {another-model-name}
```
### Colab chatbot (web)
æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªä½¿ç”¨Google Colabçš„T4/P100/V100 GPUçš„èŠå¤©æœºå™¨äººç®€å•webæ¼”ç¤ºã€‚è¯·æ³¨æ„ï¼Œæä¾›çš„gpt-neo-2.7bæ¨¡å‹æ˜¯ç›¸å¯¹è¾ƒå¼±çš„æ¨¡å‹ï¼Œä»…æ”¯æŒè‹±æ–‡ï¼Œå¹¶ä¸”æœ‰æ—¶ä¼šç”Ÿæˆä¸ç†æƒ³çš„å“åº”ã€‚


### Local Deploy
å¦‚æœæ‚¨æœ‰èµ„æºå¹¶æƒ³åœ¨æœ¬åœ°éƒ¨ç½²è‡ªå·±çš„æ¨¡å‹ï¼Œæˆ‘ä»¬ä¸ºæ‚¨æä¾›äº†ä¸€ç§ç®€å•çš„æ–¹æ³•ï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼è¿è¡Œä¸€ä¸ªFlaskæœåŠ¡å™¨æ¥å¯åŠ¨åç«¯ï¼ˆä»¥è¿›ä¸€æ­¥å‘å…¶ä»–å‰ç«¯æä¾›æœåŠ¡ï¼‰ï¼Œå¹¶æä¾›ä¸€ä¸ªäº¤äº’å¼Webå‰ç«¯ï¼ˆè®©æ‚¨ç›´æ¥å¼€å§‹èŠå¤©ï¼‰ï¼š
```sh
cd ./service
python app.py
```

## Medical Performance

|                |  PubMedQA (ID) | MedQA-USMLE (OOD) | MedMCQA (ID) |  Average |
|:---------:|:--------:|:-----------:|:-------:|:----:|
| Human (pass)   |  60.0   |     50.0    |         |      |
| Human (expert) |    78.0   |     87.0    |  90.0   | 85.0 |
|   |      |              |    |  |
|  InstructGPT 175B   |   73.2   |     46.0    |  44.0   | 54.4 |
|    ChatGPT |    63.9   |     **57.0**    |  44.7   | 55.2 |
|      LLaMA 7B   |    5.2   |     27.1    |  24.3   | 18.9 |
|      LLaMA 33B |    1.8   |     43.4    |  30.3   | 25.2 |
|   |      |             |            |    |  |
|   Task-tuned LLaMA 7B (Full) |   **75.1**   |     44.5    |  49.9   | 56.5 |
| Task-tuned LLaMA 33B (LoRA) |  74.0  |  51.3   | **50.2**|**58.5**|

LLaMA 33Bï¼ˆLoRAï¼‰çš„æ€§èƒ½ä»…ç»è¿‡çº¦16å°æ—¶çš„å¾®è°ƒï¼Œå³å¯åœ¨PubMedQAå’ŒMedMCQAçš„è®­ç»ƒé›†ä¸Šä½¿ç”¨å•ä¸ª8 * A100æœåŠ¡å™¨å®ç°ã€‚è¦äº†è§£æ›´å¤šæ€§èƒ½ä¿¡æ¯ï¼ŒåŒ…æ‹¬æŒ‡ä»¤å¾®è°ƒç»“æœï¼Œè¯·å‚è€ƒæˆ‘ä»¬çš„[Documentation](https://optimalscale.github.io/LMFlow/)

## Model Zoo
æˆ‘ä»¬å°†è®­ç»ƒå¥½çš„æ£€æŸ¥ç‚¹å¼€æºç»™æ‰€æœ‰äººè¿›è¡Œè¿›ä¸€æ­¥çš„è®­ç»ƒå’Œæ¨ç†ã€‚

| Instruct-tuned Models   |  Status | Base Model | Download | 
|----------|:-------------:|----------|:-------------:|
| LLaMA-7B-tuned | ![completed](https://geps.dev/progress/100) | LLaMA-7B | [Google Drive](https://drive.google.com/file/d/1x5JLae3akVkfFeDhSe3TEyUbPn_GNFyb/view?usp=share_link) |
| LLaMA-13B-tuned | ![completed](https://geps.dev/progress/100) | LLaMA-13B |  [Google Drive](https://drive.google.com/file/d/1m_rpe6rNpN59kWvjJ3GfKeEmS-68TRYr/view?usp=share_link) |
| LLaMA-33B-tuned | ![completed](https://geps.dev/progress/100) |LLaMA-33B |  [Google Drive](https://drive.google.com/file/d/1IqgqLHwNkWQ7BffheZnqD6a-8Zul1bk6/view?usp=share_link) |
| LLaMA-65B-tuned | ![training](https://geps.dev/progress/65) | LLaMA-65B | Google Drive |
| LLaMA7B-medical | ![completed](https://geps.dev/progress/100) | LLaMA-7B | [Google Drive](https://drive.google.com/file/d/1Z44tsrRvfDFvucbNGFjHC_vbPcBvg3x-/view?usp=share_link) |
| LLaMA13B-medical | ![completed](https://geps.dev/progress/100) | LLaMA-13B |  [Google Drive](https://drive.google.com/file/d/1uoTAXTMyYQkP6N4ummx7tj-c4v1p91ap/view?usp=share_link) |
| LLaMA33B-medical | ![completed](https://geps.dev/progress/100) |LLaMA-33B |  [Google Drive](https://drive.google.com/file/d/14N9o_1pwHmVuSikQ3orMVzZDrLYJC0iM/view?usp=share_link) |
| LLaMA65B-medical | ![training](https://geps.dev/progress/90) | LLaMA-65B | Google Drive |


## Supported Pipelines

| Pipelines   |   Status |
|----------|:-------------:|
| Task Tuning |  :white_check_mark: Supported |
| Instruction Tuning |  :white_check_mark: Supported |
| Parameter-Efficient Tuning |  :white_check_mark: Supported |
| Large Model Inference |  :white_check_mark: Supported |
| Alignment Tuning |  :wrench: Developing |



## Supported Models

æˆ‘ä»¬æ”¯æŒğŸ¤— huggingfaceä¸­çš„æ‰€æœ‰[decoder models](https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads)ï¼ŒåŒ…æ‹¬LLaMAã€GPT2ã€GPT-Neoå’ŒGalacticaç­‰ï¼Œå‡å·²è¿›è¡Œäº†å…¨é¢æµ‹è¯•ã€‚æˆ‘ä»¬å¾ˆå¿«å°†æ”¯æŒç¼–ç å™¨ï¼ˆencoder-decoderï¼‰æ¨¡å‹ã€‚



## 1.Setup
```bash
git clone https://github.com/OptimalScale/LMFlow.git
cd LMFlow
conda create -n lmflow python=3.9 -y
conda activate lmflow
conda install mpi4py
pip install -e .
```

## 2.Prepare Dataset
æ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹æŒ‡ä»¤ä¸‹è½½è®­ç»ƒæ•°æ®é›†å’Œæµ‹è¯•æ•°æ®é›†:
```bash
cd data
bash download.sh all
cd -
``` 

æ‚¨ä¹Ÿå¯ä»¥å°†æ‚¨çš„æ•°æ®è½¬æ¢æˆä»¥ä¸‹æ ¼å¼æ¥ä½¿ç”¨è‡ªå·±çš„æ•°æ®é›†:
```json
{
  "type": "text2text",
  "instances": [
    {
      "input": "Question: The Transformer architecture [START_REF]",
      "output": "N/A"
    },
    ...
  ]
}
```
```json
{
  "type": "text_only",
  "instances": [
    {
      "text": "Defintion: In this task, we ask you to write an answer to a question that involves events that may be stationary (not changing over time) or transient (changing over time). For example, the sentence \"he was born in the U.S.\" contains a stationary event since it will last forever; however, \"he is hungry\" contains a transient event since it will remain true for a short period of time. Note that a lot of the questions could have more than one correct answer. We only need a single most-likely answer. Please try to keep your \"answer\" as simple as possible. Concise and simple \"answer\" is preferred over those complex and verbose ones. \n Input: Question: Sentence: It's hail crackled across the comm, and Tara spun to retake her seat at the helm. \nQuestion: Will the hail storm ever end? \n Output: NA \n\n"
    },
    ...
  ]
}
```
## 3. Run Scripts
### 3.1 Run Finetuning

æ‚¨å¯ä»¥é€šè¿‡è¿è¡Œ `scripts/run_finetune.sh` æ¥å¾®è°ƒä¸€ä¸ªGPT-2çš„æ¨¡å‹
```sh
./scripts/run_finetune.sh
```

å¦‚æœæ‚¨æƒ³è¦æä¾›åæ˜ æ‚¨æœºå™¨è®¾ç½®çš„deepspeedå‚æ•°ï¼Œå¯ä»¥å°†ç›¸åº”çš„deepspeedå‚æ•°ä¼ é€’ç»™è„šæœ¬ã€‚ä¾‹å¦‚ï¼š
```sh
./scripts/run_finetune.sh "--num_gpus=8 --master_port 10001"
```

ä¸ºäº†å¼€å¯LoRAçš„è®­ç»ƒï¼Œæ‚¨å¯ä»¥å‚è€ƒ:
```sh
./scripts/run_finetune_with_lora.sh
```

å¦‚æœéœ€è¦è¯¦ç»†çš„é…ç½®ï¼Œå¯ä»¥ç›´æ¥ä¿®æ”¹è¿™äº›è„šæœ¬ã€‚è¿™äº›è„šæœ¬å®é™…ä¸Šåªæ˜¯è°ƒç”¨äº†pythonè„šæœ¬`examples/finetune.py`ï¼Œè¯¥è„šæœ¬å¯ä»¥æŒ‰ä»¥ä¸‹æ–¹å¼è¿è¡Œï¼š

```sh
deepspeed ${deepspeed_args} \
  examples/finetune.py \
    --deepspeed configs/ds_config_zero3.json \
    --bf16 \
    --run_name finetune_with_lora \
    --model_name_or_path facebook/galactica-1.3b \
    --num_train_epochs 0.01 \
    --learning_rate 2e-5 \
    --dataset_path ${dataset_path} \
    --per_device_train_batch_size 1 \
    --per_device_eval_batch_size 1 \
    --validation_split_percentage 0 \
    --logging_steps 20 \
    --block_size 512 \
    --do_train \
    --output_dir output_models/finetune \
    --overwrite_output_dir \
    --ddp_timeout 72000 \
    --save_steps 5000 \
    --dataloader_num_workers 1
```

```python
python examples/finetune.py -h
```

### 3.2 Run Evaluation

å¤§å®¶å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼æ¥å¯¹è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œæµ‹è¯„:
```sh
./scripts/run_evaluation.sh
```

`--model_name_or_path` æŒ‡å®šäº†æ¨¡å‹å­˜å‚¨çš„ä½ç½®ã€‚

å¯¹äºLoRAæ¨¡å‹çš„æµ‹è¯„ï¼Œæ‰§è¡Œä»¥ä¸‹æŒ‡ä»¤:
```sh
./scripts/run_evaluation_with_lora.sh
```

è¿™äº›è„šæœ¬è°ƒç”¨äº†åŸºäºæˆ‘ä»¬APIæ„å»ºçš„ç¤ºä¾‹`examples/*.py`ã€‚è¦è·å–æ›´å¤šä¸APIç›¸å…³çš„ç¤ºä¾‹ï¼Œå¯ä»¥å‚è€ƒå•å…ƒæµ‹è¯•ä¸­çš„æ–¹æ³•`tests`.



## 4. Additional Notes
### 4.1 LLaMA Checkpoint

é¦–å…ˆï¼Œæ‚¨éœ€è¦ä»[facebookresearch/llama](https://github.com/facebookresearch/llama)è·å–LLaMAæ¨¡å‹çš„è®¿é—®æƒé™ã€‚ä¸‹è½½å®˜æ–¹æ£€æŸ¥ç‚¹å¹¶å°†å…¶ä¿å­˜åˆ°`${llama-path}`ä¸­ã€‚

å…¶æ¬¡ï¼Œé€šè¿‡è¿è¡Œä»¥ä¸‹å‘½ä»¤å°†å®˜æ–¹æ£€æŸ¥ç‚¹`${llama-path}`è½¬æ¢ä¸ºHuggingFaceæ”¯æŒçš„æ£€æŸ¥ç‚¹`${llama-hf-path}`ï¼š

    `python ./scripts/convert_llama_weights_to_hf.py --input_dir ${llama-path} --model_size 7B --output_dir ${llama-hf-path}/llama-7b-hf`

ç„¶åï¼Œå°†æ£€æŸ¥ç‚¹è·¯å¾„è®¾ç½®ä¸º`${llama-hf-path}/llama-7b-hf`å³å¯å¼€å§‹ä½¿ç”¨ã€‚ç¥æ‚¨ä½¿ç”¨æ„‰å¿«ï¼

ç°åœ¨ï¼Œæ‚¨å·²ç»æ‹¥æœ‰äº†åŸå§‹çš„llama-7b-hfé¢„è®­ç»ƒæ¨¡å‹ã€‚


### 4.2 DeepSpeed Config
æ‚¨å¯ä»¥åœ¨configæ–‡ä»¶å¤¹ä¸‹è®¾ç½®DeepSpeedçš„configï¼Œå…·ä½“å¯ä»¥å‚è€ƒ [DeepSpeed Configuration](https://www.deepspeed.ai/docs/config-json/)

## 5. Model Release

### 5.1 Medical Model Checkpoints
æ‚¨å¯ä»¥è¿è¡Œä»¥ä¸‹è„šæœ¬æ¥ä¸‹è½½æˆ‘ä»¬çš„æƒé‡:

```bash
cd output_models
bash download.sh medical_ckpt
cd -
```
æ‚¨å¯ä»¥ç›´æ¥é€šè¿‡Google Driveä¸‹è½½æˆ‘ä»¬çš„æ¨¡å‹: [medical_ckpt.tar.gz](https://drive.google.com/file/d/1bnsQGNGNYchsOfiNyRAmL2fNiowbmFNw/view?usp=share_link)

### 5.2 Instruction Model Checkpoints
```bash
cd output_models
bash download.sh instruction_ckpt
cd -
```
æ‚¨å¯ä»¥ç›´æ¥é€šè¿‡Google Driveä¸‹è½½æˆ‘ä»¬çš„æ¨¡å‹: [instruction_ckpt.tar.gz](https://drive.google.com/file/d/1d_ioQ-ViVweeifbsFSO4pczc3UORFHZO/view?usp=share_link)

### 5.3 Begin Reproduce
åœ¨ä¸‹è½½äº†æ¨¡å‹Checkpointsä¹‹åï¼Œæ‚¨å¯ä»¥åœ¨`LMFlow/scripts/run_evaluation_with_lora.sh`ä¸­å°†`--lora_model_path`æ›¿æ¢ä¸º`output_models/instruction_ckpt/llama7b-lora`ï¼ˆä»¥instructionçš„llama-7bä¸ºä¾‹ï¼‰ï¼Œå¹¶å°†--model_name_or_pathæ›¿æ¢ä¸ºæ‚¨è½¬æ¢çš„llamaæ¨¡å‹ã€‚ç„¶åè¿è¡Œè¿™ä¸ªshellè„šæœ¬ä»¥é‡ç°ç»“æœã€‚

ç„¶åï¼Œæ‚¨å¯ä»¥åœ¨æˆ‘ä»¬çš„æ–‡æ¡£ä¸­æ£€æŸ¥æ¨¡å‹æ€§èƒ½ã€‚

## Documentation
è¯·å‚è€ƒæˆ‘ä»¬çš„[Documentation](https://optimalscale.github.io/LMFlow/)è·å–æ›´å¤šAPIå‚è€ƒå’Œå®éªŒç»“æœä¿¡æ¯ã€‚

## Vision
æˆ‘ä»¬å¾ˆé«˜å…´åœ°å¼€æºLMFlowä»£ç åº“ï¼Œå…¶ä¸­åŒ…æ‹¬äº†å®Œæ•´çš„å¤§æ¨¡å‹è®­ç»ƒæµç¨‹ï¼Œèƒ½å¤Ÿå¿«é€Ÿã€é«˜æ•ˆåœ°è®­ç»ƒå’Œéƒ¨ç½²è‡ªå·±çš„è¯­è¨€æ¨¡å‹ã€‚

æˆ‘ä»¬çš„ä»£ç åº“ä¸ä»…ä»…æ˜¯ä¸€ä¸ªç®€å•çš„æ¨¡å‹ï¼› å®ƒåŒ…æ‹¬å®Œæ•´çš„è®­ç»ƒæµç¨‹ã€æ¨¡å‹æƒé‡å’Œæµ‹è¯•å·¥å…·ã€‚ æ‚¨å¯ä»¥ä½¿ç”¨å®ƒæ¥æ„å»ºå„ç§ç±»å‹çš„è¯­è¨€æ¨¡å‹ï¼ŒåŒ…æ‹¬å¯¹è¯æ¨¡å‹ã€é—®ç­”æ¨¡å‹å’Œæ–‡æœ¬ç”Ÿæˆæ¨¡å‹ç­‰ã€‚

æ­¤å¤–ï¼Œæˆ‘ä»¬æ—¨åœ¨åˆ›å»ºä¸€ä¸ªå¼€æ”¾å’Œæ°‘ä¸»çš„å¤§æ¨¡å‹å…±äº«å¹³å°ï¼Œä»»ä½•äººéƒ½å¯ä»¥åœ¨è¿™ä¸ªå¹³å°ä¸Šåˆ†äº«è®­ç»ƒæ¨¡å‹æƒé‡å’Œç»éªŒã€‚ æˆ‘ä»¬æ¬¢è¿ä»»ä½•å¯¹å¤§æ¨¡å‹æ„Ÿå…´è¶£çš„äººå‚ä¸è¿›æ¥ï¼Œä¸æˆ‘ä»¬ä¸€èµ·å»ºè®¾ä¸€ä¸ªå¼€æ”¾å‹å¥½çš„ç¤¾åŒºï¼

æ— è®ºæ‚¨æ˜¯åˆå­¦è€…è¿˜æ˜¯ä¸“å®¶ï¼Œæˆ‘ä»¬ç›¸ä¿¡å¤§å®¶éƒ½èƒ½ä»è¿™ä¸ªå¹³å°ä¸­è·ç›Šã€‚è®©æˆ‘ä»¬å…±åŒåŠªåŠ›ï¼Œå»ºç«‹ä¸€ä¸ªå……æ»¡æ´»åŠ›å’Œåˆ›æ–°çš„å¤§æ¨¡å‹ç¤¾åŒºï¼

[![Embark](https://img.shields.io/badge/discord-LMFlow-%237289da.svg?logo=discord)](https://discord.gg/srGxyazbNs)
[![WeChat badge](https://img.shields.io/badge/å¾®ä¿¡-åŠ å…¥-brightgreen?logo=wechat&amp)](https://i.328888.xyz/2023/04/04/ibvpAk.jpeg)
[![slack badge](https://img.shields.io/badge/Slack-join-blueviolet?logo=slack&amp)](https://join.slack.com/t/lmflow/shared_invite/zt-1s6egx12s-THlwHuCjF6~JGKmx7JoJPA)

## Disclaimer

æ­¤è½¯ä»¶åŒ…æ—¨åœ¨ä¸ºå¤§å‹æ¨¡å‹è°ƒæ•´æä¾›ç®€åŒ–å’Œç”¨æˆ·å‹å¥½çš„æµç¨‹ã€‚å…¶åŠŸèƒ½å¯ä½œä¸ºç”¨æˆ·å‚è€ƒå¹¶ä¾›ç”¨æˆ·ä½¿ç”¨ã€‚ç„¶è€Œï¼Œéœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæ•°æ®å’Œé¢„è®­ç»ƒæ¨¡å‹çš„å‡†å¤‡å·¥ä½œå®Œå…¨ç”±ç”¨æˆ·è´Ÿè´£ã€‚æœ¬è½¯ä»¶åŒ…ä¸ä¿è¯ç”¨æˆ·å‡†å¤‡ç»„ä»¶çš„å‡†ç¡®æ€§ã€å®Œæ•´æ€§ã€é€‚ç”¨æ€§æˆ–åˆæ³•æ€§ã€‚ç”¨æˆ·å¿…é¡»äº†è§£å¹¶æ‰¿æ‹…ä¸æ¨¡å‹å’Œæ•°æ®å‡†å¤‡ç›¸å…³çš„æ‰€æœ‰é£é™©å’Œè´£ä»»ï¼Œå¹¶åœ¨ä½¿ç”¨æœ¬è½¯ä»¶åŒ…ä¹‹å‰è·å–æ³•å¾‹ã€å•†ä¸šå’ŒæŠ€æœ¯å»ºè®®ã€‚è¯¥æµç¨‹ä¸åº”å¯¹ç”¨æˆ·ä¸å½“å‡†å¤‡æ•°æ®å’Œé¢„è®­ç»ƒæ¨¡å‹æ‰€å¯¼è‡´çš„ä»»ä½•ç›´æ¥ã€é—´æ¥ã€ç‰¹æ®Šã€å¶ç„¶æˆ–åæœæ€§æŸå®³è´Ÿè´£ã€‚

æˆ‘ä»¬æä¾›çš„æ£€æŸ¥ç‚¹ä»…ä¾›ç ”ç©¶ç›®çš„ä½¿ç”¨ï¼ŒåŒ…æ‹¬è‹±æ–‡å’Œä¸­æ–‡ç‰ˆæœ¬ã€‚è¿™äº›æ£€æŸ¥ç‚¹åŒ…å«ChatGPTè¯­è¨€æ¨¡å‹ç”Ÿæˆçš„ç»“æœã€‚æˆ‘ä»¬ä¸æ”¯æŒæˆ–é¼“åŠ±å°†è¿™äº›æ£€æŸ¥ç‚¹ç”¨äºå•†ä¸šç›®çš„çš„åˆ†å‘æˆ–ä½¿ç”¨ã€‚è¿™äº›æ£€æŸ¥ç‚¹çš„ç”¨æˆ·åº”å½“è´Ÿè´£ç¡®ä¿æ­£ç¡®å’Œé€‚å½“åœ°ä½¿ç”¨å®ƒä»¬ã€‚

è¿˜éœ€è¦å¼ºè°ƒçš„æ˜¯ï¼Œæ¨¡å‹ç”Ÿæˆçš„ç»“æœæ˜¯åŸºäºæ¦‚ç‡æ¨¡å‹ï¼Œä¸æ­¤æµç¨‹æ²¡æœ‰ç›´æ¥å…³ç³»ã€‚æœ¬æµç¨‹ä¸ä¿è¯ç»“æœçš„å‡†ç¡®æ€§ã€å¯é æ€§ã€é€‚ç”¨æ€§å’Œåˆæ³•æ€§ã€‚å› æ­¤ï¼Œåœ¨ä¾èµ–æ¨¡å‹ç”Ÿæˆçš„ç»“æœä¹‹å‰ï¼Œç”¨æˆ·è¿˜å¿…é¡»äº†è§£ä¸ç»“æœç›¸å…³çš„é£é™©å’Œè´£ä»»ï¼Œå¹¶å¯»æ±‚æ³•å¾‹ã€å•†ä¸šå’ŒæŠ€æœ¯å»ºè®®ã€‚è¯¥æµç¨‹ä¸åº”å¯¹ç”¨æˆ·ä¾èµ–æ¨¡å‹ç”Ÿæˆçš„ç»“æœæ‰€å¯¼è‡´çš„ä»»ä½•ç›´æ¥ã€é—´æ¥ã€ç‰¹æ®Šã€å¶ç„¶æˆ–åæœæ€§æŸå®³è´Ÿè´£ã€‚

## Support
å¦‚æœæ‚¨éœ€è¦ä»»ä½•å¸®åŠ©ï¼Œè¯·æäº¤[Github](https://github.com/OptimalScale/LMFlow)é—®é¢˜ã€‚


## Contributors
<a href="https://github.com/OptimalScale/LMFlow/graphs/contributors">
  <img src="https://contrib.rocks/image?repo=OptimalScale/LMFlow" />
</a>

## Citation
å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„è½¯ä»¶åŒ…æœ‰ç”¨ï¼Œæ¬¢è¿ç‚¹èµâ­ã€forkã€è½¬å‘å’Œå¼•ç”¨ã€‚è°¢è°¢å¤§å®¶çš„æ”¯æŒï¼

```
@misc{lmflow,
  author = {Shizhe Diao and Rui Pan and Hanze Dong and KaShun Shum and Jipeng Zhang and Wei Xiong and Tong Zhang},
  title = {LMFlow: An Extensible Toolkit for Finetuning and Inference of Large Foundation Models},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://optimalscale.github.io/LMFlow/}},
}
```
