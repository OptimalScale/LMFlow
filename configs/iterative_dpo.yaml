# general
## model
model_name_or_path: "/home/yizhenjia/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/c4a54320a52ed5f88b7a2f84496903ea4ff07b45"  # initial model
reference_model_name_or_path: "meta-llama/Meta-Llama-3-8B-Instruct"
reward_model_name_or_path: /home/yizhenjia/projs/RLHFlow-fox/models/rm/sfairXC-FsfairX-LLaMA3-RM-v0.1
reward_arch_type: text_regression
trust_remote_code: True

## data
dataset_path_list:
  - "data/iterative-prompt-3it-100/iter1"
  - "data/iterative-prompt-3it-100/iter2"
  - "data/iterative-prompt-3it-100/iter3"
conversation_template: llama3
preprocessing_num_workers: 16

## pipeline
output_dir: ./output_models/iterative_dpo
run_name: iterative_dpo
random_seed: 42
use_accelerator: True
enable_distributed_inference: True
distributed_inference_num_instances: 8


# inference phase
## general
apply_chat_template: True
num_output_sequences: 8
use_beam_search: False
temperature: 1.0
top_p: 0.9
max_new_tokens: 4096
enable_decode_inference_result: True

## vllm
use_vllm: True
vllm_gpu_memory_utilization: 0.95
vllm_tensor_parallel_size: 1
vllm_inference_batch_size: 16


# reward model scoring phase
reward_model_inference_block_size: 2048
overwrite_cache: True
reward_model_inference_batch_size: 3 # the actual batch size for rm forward will be reward_model_inference_batch_size * num_output_sequences


# dpo phase
## model
do_train: True

## data
sampling_paired_method: max_min
margin_scale: 1.0
length_penalty: 0
max_prompt_length: 1000
mask_prompt: True

## pipeline
### training
bf16: True
num_train_epochs: 2
learning_rate: 5e-7
warmup_steps: 100
per_device_train_batch_size: 1
per_device_eval_batch_size: 1
gradient_accumulation_steps: 16
gradient_checkpointing: True
loss_type: sigmoid
lr_scheduler_type: cosine
optim: paged_adamw_32bit

### logging
logging_steps: 2
save_strategy: steps
save_steps: 100
evaluation_strategy: steps
eval_steps: 100
report_to: wandb