packaging
numpy==1.24.2
datasets==2.14.6
tokenizers>=0.13.3
peft>=0.10.0
torch>=2.0.1
wandb==0.14.0
deepspeed<=0.14.0
trl==0.8.0
sentencepiece
transformers>=4.31.0
flask
flask_cors
icetk
cpm_kernels==1.0.11
evaluate==0.4.0
scikit-learn==1.2.2
lm-eval==0.3.0
dill<0.3.5
bitsandbytes>=0.40.0
pydantic
gradio
accelerate>=0.27.2
einops>=0.6.1
vllm>=0.4.1
diffusers>=0.29.2